
Starting Fold 1:
Train samples: 13884, Test samples: 3472

Epoch 5: train loss (last batch), mean MSE:	1.4457,	1.44668
Epoch 5: valid loss (last batch), mean MSE:	1.2247,	1.37696
Epoch 5: valid baseline mean MSE:	0.46943

Epoch 10: train loss (last batch), mean MSE:	0.9862,	0.95831
Epoch 10: valid loss (last batch), mean MSE:	0.9156,	0.92615
Epoch 10: valid baseline mean MSE:	0.46882

Epoch 15: train loss (last batch), mean MSE:	0.6991,	0.71882
Epoch 15: valid loss (last batch), mean MSE:	0.6487,	0.70225
Epoch 15: valid baseline mean MSE:	0.46873

Epoch 20: train loss (last batch), mean MSE:	0.5878,	0.59828
Epoch 20: valid loss (last batch), mean MSE:	0.5605,	0.59019
Epoch 20: valid baseline mean MSE:	0.46777

Epoch 25: train loss (last batch), mean MSE:	0.4831,	0.53787
Epoch 25: valid loss (last batch), mean MSE:	0.4730,	0.53349
Epoch 25: valid baseline mean MSE:	0.46830

Epoch 30: train loss (last batch), mean MSE:	0.4632,	0.50773
Epoch 30: valid loss (last batch), mean MSE:	0.4053,	0.50487
Epoch 30: valid baseline mean MSE:	0.46648

Epoch 35: train loss (last batch), mean MSE:	0.4568,	0.49250
Epoch 35: valid loss (last batch), mean MSE:	0.4554,	0.49152
Epoch 35: valid baseline mean MSE:	0.46819

Epoch 40: train loss (last batch), mean MSE:	0.4366,	0.48465
Epoch 40: valid loss (last batch), mean MSE:	0.4204,	0.48380
Epoch 40: valid baseline mean MSE:	0.46846

Epoch 45: train loss (last batch), mean MSE:	0.4925,	0.48056
Epoch 45: valid loss (last batch), mean MSE:	0.5330,	0.48156
Epoch 45: valid baseline mean MSE:	0.46952

Epoch 50: train loss (last batch), mean MSE:	0.5044,	0.47845
Epoch 50: valid loss (last batch), mean MSE:	0.5009,	0.47914
Epoch 50: valid baseline mean MSE:	0.46921

Epoch 55: train loss (last batch), mean MSE:	0.4659,	0.47736
Epoch 55: valid loss (last batch), mean MSE:	0.4660,	0.47769
Epoch 55: valid baseline mean MSE:	0.46925

Epoch 60: train loss (last batch), mean MSE:	0.4354,	0.47680
Epoch 60: valid loss (last batch), mean MSE:	0.4826,	0.47740
Epoch 60: valid baseline mean MSE:	0.46871

Epoch 65: train loss (last batch), mean MSE:	0.5485,	0.47655
Epoch 65: valid loss (last batch), mean MSE:	0.5137,	0.47757
Epoch 65: valid baseline mean MSE:	0.46971

Epoch 70: train loss (last batch), mean MSE:	0.4730,	0.47638
Epoch 70: valid loss (last batch), mean MSE:	0.4877,	0.47707
Epoch 70: valid baseline mean MSE:	0.46958

Epoch 75: train loss (last batch), mean MSE:	0.4747,	0.47631
Epoch 75: valid loss (last batch), mean MSE:	0.5281,	0.47756
Epoch 75: valid baseline mean MSE:	0.46893

Epoch 80: train loss (last batch), mean MSE:	0.4800,	0.47630
Epoch 80: valid loss (last batch), mean MSE:	0.4065,	0.47590
Epoch 80: valid baseline mean MSE:	0.46831

Epoch 85: train loss (last batch), mean MSE:	0.4518,	0.47629
Epoch 85: valid loss (last batch), mean MSE:	0.5062,	0.47726
Epoch 85: valid baseline mean MSE:	0.46975

Epoch 90: train loss (last batch), mean MSE:	0.4652,	0.47630
Epoch 90: valid loss (last batch), mean MSE:	0.4458,	0.47643
Epoch 90: valid baseline mean MSE:	0.46881

Epoch 95: train loss (last batch), mean MSE:	0.4557,	0.47629
Epoch 95: valid loss (last batch), mean MSE:	0.4912,	0.47705
Epoch 95: valid baseline mean MSE:	0.46961

Epoch 100: train loss (last batch), mean MSE:	0.4850,	0.47629
Epoch 100: valid loss (last batch), mean MSE:	0.4316,	0.47622
Epoch 100: valid baseline mean MSE:	0.46888

Starting Fold 2:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.4009,	1.44739
Epoch 5: valid loss (last batch), mean MSE:	1.5410,	1.37640
Epoch 5: valid baseline mean MSE:	0.46876

Epoch 10: train loss (last batch), mean MSE:	0.9697,	0.95875
Epoch 10: valid loss (last batch), mean MSE:	0.9046,	0.92325
Epoch 10: valid baseline mean MSE:	0.46876

Epoch 15: train loss (last batch), mean MSE:	0.7139,	0.71914
Epoch 15: valid loss (last batch), mean MSE:	0.7218,	0.70168
Epoch 15: valid baseline mean MSE:	0.46876

Epoch 20: train loss (last batch), mean MSE:	0.6060,	0.59848
Epoch 20: valid loss (last batch), mean MSE:	0.5463,	0.58906
Epoch 20: valid baseline mean MSE:	0.46903

Epoch 25: train loss (last batch), mean MSE:	0.5304,	0.53798
Epoch 25: valid loss (last batch), mean MSE:	0.5187,	0.53354
Epoch 25: valid baseline mean MSE:	0.46881

Epoch 30: train loss (last batch), mean MSE:	0.5326,	0.50778
Epoch 30: valid loss (last batch), mean MSE:	0.4870,	0.50557
Epoch 30: valid baseline mean MSE:	0.46808

Epoch 35: train loss (last batch), mean MSE:	0.4913,	0.49254
Epoch 35: valid loss (last batch), mean MSE:	0.5797,	0.49296
Epoch 35: valid baseline mean MSE:	0.46940

Epoch 40: train loss (last batch), mean MSE:	0.4530,	0.48469
Epoch 40: valid loss (last batch), mean MSE:	0.4432,	0.48389
Epoch 40: valid baseline mean MSE:	0.46821

Epoch 45: train loss (last batch), mean MSE:	0.4843,	0.48060
Epoch 45: valid loss (last batch), mean MSE:	0.4366,	0.48005
Epoch 45: valid baseline mean MSE:	0.46818

Epoch 50: train loss (last batch), mean MSE:	0.4948,	0.47849
Epoch 50: valid loss (last batch), mean MSE:	0.4496,	0.47829
Epoch 50: valid baseline mean MSE:	0.46855

Epoch 55: train loss (last batch), mean MSE:	0.5053,	0.47740
Epoch 55: valid loss (last batch), mean MSE:	0.4465,	0.47728
Epoch 55: valid baseline mean MSE:	0.46877

Epoch 60: train loss (last batch), mean MSE:	0.4372,	0.47683
Epoch 60: valid loss (last batch), mean MSE:	0.4390,	0.47665
Epoch 60: valid baseline mean MSE:	0.46883

Epoch 65: train loss (last batch), mean MSE:	0.4652,	0.47656
Epoch 65: valid loss (last batch), mean MSE:	0.3984,	0.47583
Epoch 65: valid baseline mean MSE:	0.46774

Epoch 70: train loss (last batch), mean MSE:	0.5130,	0.47643
Epoch 70: valid loss (last batch), mean MSE:	0.4072,	0.47582
Epoch 70: valid baseline mean MSE:	0.46813

Epoch 75: train loss (last batch), mean MSE:	0.4404,	0.47634
Epoch 75: valid loss (last batch), mean MSE:	0.5104,	0.47721
Epoch 75: valid baseline mean MSE:	0.46896

Epoch 80: train loss (last batch), mean MSE:	0.4878,	0.47633
Epoch 80: valid loss (last batch), mean MSE:	0.5206,	0.47735
Epoch 80: valid baseline mean MSE:	0.46881

Epoch 85: train loss (last batch), mean MSE:	0.4846,	0.47633
Epoch 85: valid loss (last batch), mean MSE:	0.5540,	0.47779
Epoch 85: valid baseline mean MSE:	0.47019

Epoch 90: train loss (last batch), mean MSE:	0.4640,	0.47633
Epoch 90: valid loss (last batch), mean MSE:	0.5272,	0.47741
Epoch 90: valid baseline mean MSE:	0.46952

Epoch 95: train loss (last batch), mean MSE:	0.4921,	0.47633
Epoch 95: valid loss (last batch), mean MSE:	0.4856,	0.47682
Epoch 95: valid baseline mean MSE:	0.46869

Epoch 100: train loss (last batch), mean MSE:	0.5084,	0.47634
Epoch 100: valid loss (last batch), mean MSE:	0.4311,	0.47608
Epoch 100: valid baseline mean MSE:	0.46870

Starting Fold 3:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.3438,	1.44716
Epoch 5: valid loss (last batch), mean MSE:	1.3037,	1.37550
Epoch 5: valid baseline mean MSE:	0.47016

Epoch 10: train loss (last batch), mean MSE:	0.9251,	0.95827
Epoch 10: valid loss (last batch), mean MSE:	0.9240,	0.92490
Epoch 10: valid baseline mean MSE:	0.46990

Epoch 15: train loss (last batch), mean MSE:	0.7304,	0.71880
Epoch 15: valid loss (last batch), mean MSE:	0.6977,	0.70275
Epoch 15: valid baseline mean MSE:	0.47041

Epoch 20: train loss (last batch), mean MSE:	0.5767,	0.59820
Epoch 20: valid loss (last batch), mean MSE:	0.5790,	0.59093
Epoch 20: valid baseline mean MSE:	0.47018

Epoch 25: train loss (last batch), mean MSE:	0.5384,	0.53772
Epoch 25: valid loss (last batch), mean MSE:	0.5810,	0.53587
Epoch 25: valid baseline mean MSE:	0.47033

Epoch 30: train loss (last batch), mean MSE:	0.5317,	0.50750
Epoch 30: valid loss (last batch), mean MSE:	0.4527,	0.50658
Epoch 30: valid baseline mean MSE:	0.46950

Epoch 35: train loss (last batch), mean MSE:	0.4896,	0.49223
Epoch 35: valid loss (last batch), mean MSE:	0.4878,	0.49315
Epoch 35: valid baseline mean MSE:	0.47024

Epoch 40: train loss (last batch), mean MSE:	0.4816,	0.48436
Epoch 40: valid loss (last batch), mean MSE:	0.4491,	0.48542
Epoch 40: valid baseline mean MSE:	0.46936

Epoch 45: train loss (last batch), mean MSE:	0.4889,	0.48024
Epoch 45: valid loss (last batch), mean MSE:	0.4705,	0.48195
Epoch 45: valid baseline mean MSE:	0.46988

Epoch 50: train loss (last batch), mean MSE:	0.4934,	0.47811
Epoch 50: valid loss (last batch), mean MSE:	0.4385,	0.47956
Epoch 50: valid baseline mean MSE:	0.47021

Epoch 55: train loss (last batch), mean MSE:	0.4959,	0.47702
Epoch 55: valid loss (last batch), mean MSE:	0.4820,	0.47918
Epoch 55: valid baseline mean MSE:	0.47049

Epoch 60: train loss (last batch), mean MSE:	0.4813,	0.47647
Epoch 60: valid loss (last batch), mean MSE:	0.4886,	0.47877
Epoch 60: valid baseline mean MSE:	0.47031

Epoch 65: train loss (last batch), mean MSE:	0.4678,	0.47619
Epoch 65: valid loss (last batch), mean MSE:	0.5226,	0.47899
Epoch 65: valid baseline mean MSE:	0.47104

Epoch 70: train loss (last batch), mean MSE:	0.4868,	0.47605
Epoch 70: valid loss (last batch), mean MSE:	0.5017,	0.47857
Epoch 70: valid baseline mean MSE:	0.47071

Epoch 75: train loss (last batch), mean MSE:	0.4303,	0.47598
Epoch 75: valid loss (last batch), mean MSE:	0.5866,	0.47970
Epoch 75: valid baseline mean MSE:	0.47192

Epoch 80: train loss (last batch), mean MSE:	0.5188,	0.47598
Epoch 80: valid loss (last batch), mean MSE:	0.4573,	0.47788
Epoch 80: valid baseline mean MSE:	0.46925

Epoch 85: train loss (last batch), mean MSE:	0.4741,	0.47597
Epoch 85: valid loss (last batch), mean MSE:	0.4330,	0.47754
Epoch 85: valid baseline mean MSE:	0.46994

Epoch 90: train loss (last batch), mean MSE:	0.4818,	0.47597
Epoch 90: valid loss (last batch), mean MSE:	0.4100,	0.47722
Epoch 90: valid baseline mean MSE:	0.46972

Epoch 95: train loss (last batch), mean MSE:	0.4954,	0.47597
Epoch 95: valid loss (last batch), mean MSE:	0.5319,	0.47892
Epoch 95: valid baseline mean MSE:	0.47089

Epoch 100: train loss (last batch), mean MSE:	0.5484,	0.47598
Epoch 100: valid loss (last batch), mean MSE:	0.4681,	0.47803
Epoch 100: valid baseline mean MSE:	0.46992

Starting Fold 4:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.3872,	1.44742
Epoch 5: valid loss (last batch), mean MSE:	1.3625,	1.37739
Epoch 5: valid baseline mean MSE:	0.46799

Epoch 10: train loss (last batch), mean MSE:	0.9059,	0.95859
Epoch 10: valid loss (last batch), mean MSE:	0.8866,	0.92406
Epoch 10: valid baseline mean MSE:	0.46666

Epoch 15: train loss (last batch), mean MSE:	0.7230,	0.71922
Epoch 15: valid loss (last batch), mean MSE:	0.7625,	0.70216
Epoch 15: valid baseline mean MSE:	0.46808

Epoch 20: train loss (last batch), mean MSE:	0.6194,	0.59874
Epoch 20: valid loss (last batch), mean MSE:	0.6210,	0.58927
Epoch 20: valid baseline mean MSE:	0.46791

Epoch 25: train loss (last batch), mean MSE:	0.5457,	0.53832
Epoch 25: valid loss (last batch), mean MSE:	0.5470,	0.53266
Epoch 25: valid baseline mean MSE:	0.46638

Epoch 30: train loss (last batch), mean MSE:	0.4836,	0.50816
Epoch 30: valid loss (last batch), mean MSE:	0.4442,	0.50346
Epoch 30: valid baseline mean MSE:	0.46593

Epoch 35: train loss (last batch), mean MSE:	0.4988,	0.49296
Epoch 35: valid loss (last batch), mean MSE:	0.5106,	0.49035
Epoch 35: valid baseline mean MSE:	0.46760

Epoch 40: train loss (last batch), mean MSE:	0.4914,	0.48512
Epoch 40: valid loss (last batch), mean MSE:	0.4391,	0.48211
Epoch 40: valid baseline mean MSE:	0.46614

Epoch 45: train loss (last batch), mean MSE:	0.4567,	0.48102
Epoch 45: valid loss (last batch), mean MSE:	0.5517,	0.47989
Epoch 45: valid baseline mean MSE:	0.46780

Epoch 50: train loss (last batch), mean MSE:	0.4616,	0.47890
Epoch 50: valid loss (last batch), mean MSE:	0.5878,	0.47844
Epoch 50: valid baseline mean MSE:	0.46858

Epoch 55: train loss (last batch), mean MSE:	0.4893,	0.47782
Epoch 55: valid loss (last batch), mean MSE:	0.4775,	0.47592
Epoch 55: valid baseline mean MSE:	0.46747

Epoch 60: train loss (last batch), mean MSE:	0.4611,	0.47728
Epoch 60: valid loss (last batch), mean MSE:	0.4409,	0.47492
Epoch 60: valid baseline mean MSE:	0.46635

Epoch 65: train loss (last batch), mean MSE:	0.4645,	0.47700
Epoch 65: valid loss (last batch), mean MSE:	0.5467,	0.47612
Epoch 65: valid baseline mean MSE:	0.46757

Epoch 70: train loss (last batch), mean MSE:	0.4876,	0.47686
Epoch 70: valid loss (last batch), mean MSE:	0.4629,	0.47483
Epoch 70: valid baseline mean MSE:	0.46669

Epoch 75: train loss (last batch), mean MSE:	0.5136,	0.47680
Epoch 75: valid loss (last batch), mean MSE:	0.4644,	0.47479
Epoch 75: valid baseline mean MSE:	0.46639

Epoch 80: train loss (last batch), mean MSE:	0.4808,	0.47678
Epoch 80: valid loss (last batch), mean MSE:	0.4588,	0.47469
Epoch 80: valid baseline mean MSE:	0.46712

Epoch 85: train loss (last batch), mean MSE:	0.4818,	0.47678
Epoch 85: valid loss (last batch), mean MSE:	0.5831,	0.47643
Epoch 85: valid baseline mean MSE:	0.46870

Epoch 90: train loss (last batch), mean MSE:	0.5158,	0.47679
Epoch 90: valid loss (last batch), mean MSE:	0.4109,	0.47403
Epoch 90: valid baseline mean MSE:	0.46652

Epoch 95: train loss (last batch), mean MSE:	0.4549,	0.47677
Epoch 95: valid loss (last batch), mean MSE:	0.4453,	0.47452
Epoch 95: valid baseline mean MSE:	0.46716

Epoch 100: train loss (last batch), mean MSE:	0.4868,	0.47678
Epoch 100: valid loss (last batch), mean MSE:	0.4379,	0.47440
Epoch 100: valid baseline mean MSE:	0.46568

Starting Fold 5:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.3447,	1.44457
Epoch 5: valid loss (last batch), mean MSE:	1.1810,	1.38169
Epoch 5: valid baseline mean MSE:	0.46725

Epoch 10: train loss (last batch), mean MSE:	0.9332,	0.95740
Epoch 10: valid loss (last batch), mean MSE:	1.0243,	0.93060
Epoch 10: valid baseline mean MSE:	0.46776

Epoch 15: train loss (last batch), mean MSE:	0.7213,	0.71866
Epoch 15: valid loss (last batch), mean MSE:	0.6911,	0.70424
Epoch 15: valid baseline mean MSE:	0.46602

Epoch 20: train loss (last batch), mean MSE:	0.5979,	0.59848
Epoch 20: valid loss (last batch), mean MSE:	0.5845,	0.59078
Epoch 20: valid baseline mean MSE:	0.46743

Epoch 25: train loss (last batch), mean MSE:	0.5504,	0.53822
Epoch 25: valid loss (last batch), mean MSE:	0.5493,	0.53404
Epoch 25: valid baseline mean MSE:	0.46749

Epoch 30: train loss (last batch), mean MSE:	0.4830,	0.50810
Epoch 30: valid loss (last batch), mean MSE:	0.4584,	0.50458
Epoch 30: valid baseline mean MSE:	0.46698

Epoch 35: train loss (last batch), mean MSE:	0.5344,	0.49290
Epoch 35: valid loss (last batch), mean MSE:	0.5255,	0.49125
Epoch 35: valid baseline mean MSE:	0.46844

Epoch 40: train loss (last batch), mean MSE:	0.4880,	0.48505
Epoch 40: valid loss (last batch), mean MSE:	0.4151,	0.48235
Epoch 40: valid baseline mean MSE:	0.46704

Epoch 45: train loss (last batch), mean MSE:	0.4810,	0.48096
Epoch 45: valid loss (last batch), mean MSE:	0.4456,	0.47895
Epoch 45: valid baseline mean MSE:	0.46687

Epoch 50: train loss (last batch), mean MSE:	0.4646,	0.47883
Epoch 50: valid loss (last batch), mean MSE:	0.4971,	0.47764
Epoch 50: valid baseline mean MSE:	0.46768

Epoch 55: train loss (last batch), mean MSE:	0.4300,	0.47772
Epoch 55: valid loss (last batch), mean MSE:	0.5058,	0.47675
Epoch 55: valid baseline mean MSE:	0.46779

Epoch 60: train loss (last batch), mean MSE:	0.4734,	0.47719
Epoch 60: valid loss (last batch), mean MSE:	0.4236,	0.47508
Epoch 60: valid baseline mean MSE:	0.46696

Epoch 65: train loss (last batch), mean MSE:	0.5132,	0.47691
Epoch 65: valid loss (last batch), mean MSE:	0.4518,	0.47520
Epoch 65: valid baseline mean MSE:	0.46665

Epoch 70: train loss (last batch), mean MSE:	0.4805,	0.47677
Epoch 70: valid loss (last batch), mean MSE:	0.4769,	0.47544
Epoch 70: valid baseline mean MSE:	0.46766

Epoch 75: train loss (last batch), mean MSE:	0.4693,	0.47670
Epoch 75: valid loss (last batch), mean MSE:	0.4136,	0.47448
Epoch 75: valid baseline mean MSE:	0.46675

Epoch 80: train loss (last batch), mean MSE:	0.4759,	0.47667
Epoch 80: valid loss (last batch), mean MSE:	0.4173,	0.47454
Epoch 80: valid baseline mean MSE:	0.46700

Epoch 85: train loss (last batch), mean MSE:	0.4909,	0.47668
Epoch 85: valid loss (last batch), mean MSE:	0.5825,	0.47682
Epoch 85: valid baseline mean MSE:	0.46928

Epoch 90: train loss (last batch), mean MSE:	0.4637,	0.47667
Epoch 90: valid loss (last batch), mean MSE:	0.4330,	0.47476
Epoch 90: valid baseline mean MSE:	0.46711

Epoch 95: train loss (last batch), mean MSE:	0.4520,	0.47667
Epoch 95: valid loss (last batch), mean MSE:	0.4338,	0.47477
Epoch 95: valid baseline mean MSE:	0.46680

Epoch 100: train loss (last batch), mean MSE:	0.4399,	0.47668
Epoch 100: valid loss (last batch), mean MSE:	0.4440,	0.47491
Epoch 100: valid baseline mean MSE:	0.46654

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19797001: <VAE_to_AE> in cluster <dcc> Done

Job <VAE_to_AE> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Sun Dec 17 13:12:47 2023
Job was executed on host(s) <n-62-20-11>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Sun Dec 17 13:13:08 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/final_scripts> was used as the working directory.
Started at Sun Dec 17 13:13:08 2023
Terminated at Sun Dec 17 15:51:42 2023
Results reported at Sun Dec 17 15:51:42 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J VAE_to_AE 
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o FFNN_100.out
#BSUB -e FFNN_100.err

module load python3/3.11.4
module load h5py
python3 "FFNN_VAE_to_AE.py"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   35805.96 sec.
    Max Memory :                                 764 MB
    Average Memory :                             629.07 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               32004.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   9514 sec.
    Turnaround time :                            9535 sec.

The output (if any) is above this job summary.



PS:

Read file <FFNN_100.err> for stderr output of this job.

