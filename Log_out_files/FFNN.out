
------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19706239: <FFNN_baseline> in cluster <dcc> Exited

Job <FFNN_baseline> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Wed Dec  6 14:39:38 2023
Job was executed on host(s) <n-62-20-14>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Wed Dec  6 14:39:40 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/scripts_NN> was used as the working directory.
Started at Wed Dec  6 14:39:40 2023
Terminated at Wed Dec  6 14:39:42 2023
Results reported at Wed Dec  6 14:39:42 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J FFNN_baseline
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o ../Log_out_files/FFNN.out
#BSUB -e ../Log_out_files/FFNN.err

module load python3/3.11.4
module load h5py
python3 "scripts_NN/CSL_FFNN_BS_1HL.py"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   0.37 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   45 sec.
    Turnaround time :                            4 sec.

The output (if any) is above this job summary.



PS:

Read file <../Log_out_files/FFNN.err> for stderr output of this job.


Starting Fold 1:
Train samples: 13884, Test samples: 3472

Epoch 5: train loss (last batch), mean MSE:	0.4629,	0.48815
Epoch 5: valid loss (last batch), mean MSE:	0.6035,	0.48573
Epoch 5: valid baseline mean MSE:	0.47048

Epoch 10: train loss (last batch), mean MSE:	0.5147,	0.47664
Epoch 10: valid loss (last batch), mean MSE:	0.4885,	0.47725
Epoch 10: valid baseline mean MSE:	0.46948

Epoch 15: train loss (last batch), mean MSE:	0.4591,	0.47658
Epoch 15: valid loss (last batch), mean MSE:	0.4652,	0.47703
Epoch 15: valid baseline mean MSE:	0.46889

Epoch 20: train loss (last batch), mean MSE:	0.5031,	0.47655
Epoch 20: valid loss (last batch), mean MSE:	0.4208,	0.47634
Epoch 20: valid baseline mean MSE:	0.46866

Epoch 25: train loss (last batch), mean MSE:	0.4867,	0.47664
Epoch 25: valid loss (last batch), mean MSE:	0.4633,	0.47690
Epoch 25: valid baseline mean MSE:	0.46941

Epoch 30: train loss (last batch), mean MSE:	0.4829,	0.47660
Epoch 30: valid loss (last batch), mean MSE:	0.4574,	0.47682
Epoch 30: valid baseline mean MSE:	0.46903

Epoch 35: train loss (last batch), mean MSE:	0.4803,	0.47663
Epoch 35: valid loss (last batch), mean MSE:	0.5107,	0.47755
Epoch 35: valid baseline mean MSE:	0.46917

Epoch 40: train loss (last batch), mean MSE:	0.4904,	0.47656
Epoch 40: valid loss (last batch), mean MSE:	0.4103,	0.47646
Epoch 40: valid baseline mean MSE:	0.46855

Epoch 45: train loss (last batch), mean MSE:	0.4274,	0.47660
Epoch 45: valid loss (last batch), mean MSE:	0.4581,	0.47685
Epoch 45: valid baseline mean MSE:	0.46899

Epoch 50: train loss (last batch), mean MSE:	0.5052,	0.47663
Epoch 50: valid loss (last batch), mean MSE:	0.4817,	0.47719
Epoch 50: valid baseline mean MSE:	0.46940

Starting Fold 2:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4990,	0.48826
Epoch 5: valid loss (last batch), mean MSE:	0.4813,	0.48393
Epoch 5: valid baseline mean MSE:	0.46809

Epoch 10: train loss (last batch), mean MSE:	0.4578,	0.47666
Epoch 10: valid loss (last batch), mean MSE:	0.4050,	0.47594
Epoch 10: valid baseline mean MSE:	0.46817

Epoch 15: train loss (last batch), mean MSE:	0.4963,	0.47661
Epoch 15: valid loss (last batch), mean MSE:	0.4793,	0.47693
Epoch 15: valid baseline mean MSE:	0.46882

Epoch 20: train loss (last batch), mean MSE:	0.4740,	0.47665
Epoch 20: valid loss (last batch), mean MSE:	0.4878,	0.47711
Epoch 20: valid baseline mean MSE:	0.46866

Epoch 25: train loss (last batch), mean MSE:	0.4608,	0.47668
Epoch 25: valid loss (last batch), mean MSE:	0.4262,	0.47621
Epoch 25: valid baseline mean MSE:	0.46802

Epoch 30: train loss (last batch), mean MSE:	0.4476,	0.47669
Epoch 30: valid loss (last batch), mean MSE:	0.4773,	0.47707
Epoch 30: valid baseline mean MSE:	0.46832

Epoch 35: train loss (last batch), mean MSE:	0.5260,	0.47664
Epoch 35: valid loss (last batch), mean MSE:	0.4820,	0.47733
Epoch 35: valid baseline mean MSE:	0.46917

Epoch 40: train loss (last batch), mean MSE:	0.4408,	0.47659
Epoch 40: valid loss (last batch), mean MSE:	0.4449,	0.47644
Epoch 40: valid baseline mean MSE:	0.46811

Epoch 45: train loss (last batch), mean MSE:	0.4593,	0.47661
Epoch 45: valid loss (last batch), mean MSE:	0.4761,	0.47698
Epoch 45: valid baseline mean MSE:	0.46920

Epoch 50: train loss (last batch), mean MSE:	0.5101,	0.47666
Epoch 50: valid loss (last batch), mean MSE:	0.4911,	0.47717
Epoch 50: valid baseline mean MSE:	0.46913

Starting Fold 3:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4476,	0.48781
Epoch 5: valid loss (last batch), mean MSE:	0.6680,	0.48787
Epoch 5: valid baseline mean MSE:	0.47195

Epoch 10: train loss (last batch), mean MSE:	0.4741,	0.47633
Epoch 10: valid loss (last batch), mean MSE:	0.5112,	0.47885
Epoch 10: valid baseline mean MSE:	0.47054

Epoch 15: train loss (last batch), mean MSE:	0.4738,	0.47624
Epoch 15: valid loss (last batch), mean MSE:	0.4662,	0.47820
Epoch 15: valid baseline mean MSE:	0.47034

Epoch 20: train loss (last batch), mean MSE:	0.4729,	0.47624
Epoch 20: valid loss (last batch), mean MSE:	0.4301,	0.47778
Epoch 20: valid baseline mean MSE:	0.46983

Epoch 25: train loss (last batch), mean MSE:	0.5186,	0.47631
Epoch 25: valid loss (last batch), mean MSE:	0.4611,	0.47818
Epoch 25: valid baseline mean MSE:	0.47035

Epoch 30: train loss (last batch), mean MSE:	0.5233,	0.47627
Epoch 30: valid loss (last batch), mean MSE:	0.5438,	0.47936
Epoch 30: valid baseline mean MSE:	0.47119

Epoch 35: train loss (last batch), mean MSE:	0.4583,	0.47625
Epoch 35: valid loss (last batch), mean MSE:	0.4644,	0.47842
Epoch 35: valid baseline mean MSE:	0.47048

Epoch 40: train loss (last batch), mean MSE:	0.4651,	0.47629
Epoch 40: valid loss (last batch), mean MSE:	0.4248,	0.47766
Epoch 40: valid baseline mean MSE:	0.46938

Epoch 45: train loss (last batch), mean MSE:	0.4922,	0.47626
Epoch 45: valid loss (last batch), mean MSE:	0.4174,	0.47759
Epoch 45: valid baseline mean MSE:	0.46994

Epoch 50: train loss (last batch), mean MSE:	0.4531,	0.47625
Epoch 50: valid loss (last batch), mean MSE:	0.4604,	0.47827
Epoch 50: valid baseline mean MSE:	0.47013

Starting Fold 4:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4311,	0.48863
Epoch 5: valid loss (last batch), mean MSE:	0.4265,	0.48152
Epoch 5: valid baseline mean MSE:	0.46631

Epoch 10: train loss (last batch), mean MSE:	0.4624,	0.47713
Epoch 10: valid loss (last batch), mean MSE:	0.4545,	0.47495
Epoch 10: valid baseline mean MSE:	0.46710

Epoch 15: train loss (last batch), mean MSE:	0.4709,	0.47707
Epoch 15: valid loss (last batch), mean MSE:	0.4200,	0.47440
Epoch 15: valid baseline mean MSE:	0.46582

Epoch 20: train loss (last batch), mean MSE:	0.4818,	0.47706
Epoch 20: valid loss (last batch), mean MSE:	0.4414,	0.47466
Epoch 20: valid baseline mean MSE:	0.46661

Epoch 25: train loss (last batch), mean MSE:	0.4910,	0.47708
Epoch 25: valid loss (last batch), mean MSE:	0.4131,	0.47437
Epoch 25: valid baseline mean MSE:	0.46572

Epoch 30: train loss (last batch), mean MSE:	0.4865,	0.47709
Epoch 30: valid loss (last batch), mean MSE:	0.4716,	0.47510
Epoch 30: valid baseline mean MSE:	0.46659

Epoch 35: train loss (last batch), mean MSE:	0.4662,	0.47710
Epoch 35: valid loss (last batch), mean MSE:	0.4786,	0.47518
Epoch 35: valid baseline mean MSE:	0.46652

Epoch 40: train loss (last batch), mean MSE:	0.4851,	0.47708
Epoch 40: valid loss (last batch), mean MSE:	0.4111,	0.47428
Epoch 40: valid baseline mean MSE:	0.46730

Epoch 45: train loss (last batch), mean MSE:	0.4831,	0.47707
Epoch 45: valid loss (last batch), mean MSE:	0.4843,	0.47536
Epoch 45: valid baseline mean MSE:	0.46710

Epoch 50: train loss (last batch), mean MSE:	0.4651,	0.47714
Epoch 50: valid loss (last batch), mean MSE:	0.4598,	0.47496
Epoch 50: valid baseline mean MSE:	0.46751

Starting Fold 5:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4667,	0.48854
Epoch 5: valid loss (last batch), mean MSE:	0.4610,	0.48248
Epoch 5: valid baseline mean MSE:	0.46755

Epoch 10: train loss (last batch), mean MSE:	0.4984,	0.47706
Epoch 10: valid loss (last batch), mean MSE:	0.4507,	0.47521
Epoch 10: valid baseline mean MSE:	0.46643

Epoch 15: train loss (last batch), mean MSE:	0.4937,	0.47693
Epoch 15: valid loss (last batch), mean MSE:	0.5194,	0.47612
Epoch 15: valid baseline mean MSE:	0.46778

Epoch 20: train loss (last batch), mean MSE:	0.4710,	0.47697
Epoch 20: valid loss (last batch), mean MSE:	0.5987,	0.47721
Epoch 20: valid baseline mean MSE:	0.46917

Epoch 25: train loss (last batch), mean MSE:	0.4736,	0.47696
Epoch 25: valid loss (last batch), mean MSE:	0.4674,	0.47546
Epoch 25: valid baseline mean MSE:	0.46749

Epoch 30: train loss (last batch), mean MSE:	0.4563,	0.47699
Epoch 30: valid loss (last batch), mean MSE:	0.5836,	0.47706
Epoch 30: valid baseline mean MSE:	0.46906

Epoch 35: train loss (last batch), mean MSE:	0.4932,	0.47700
Epoch 35: valid loss (last batch), mean MSE:	0.4074,	0.47476
Epoch 35: valid baseline mean MSE:	0.46645

Epoch 40: train loss (last batch), mean MSE:	0.4661,	0.47702
Epoch 40: valid loss (last batch), mean MSE:	0.4526,	0.47530
Epoch 40: valid baseline mean MSE:	0.46706

Epoch 45: train loss (last batch), mean MSE:	0.4371,	0.47699
Epoch 45: valid loss (last batch), mean MSE:	0.4572,	0.47535
Epoch 45: valid baseline mean MSE:	0.46700

Epoch 50: train loss (last batch), mean MSE:	0.4842,	0.47702
Epoch 50: valid loss (last batch), mean MSE:	0.5178,	0.47629
Epoch 50: valid baseline mean MSE:	0.46800

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19706334: <FFNN_baseline> in cluster <dcc> Done

Job <FFNN_baseline> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Wed Dec  6 14:42:21 2023
Job was executed on host(s) <n-62-20-14>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Wed Dec  6 14:42:22 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/scripts_NN> was used as the working directory.
Started at Wed Dec  6 14:42:22 2023
Terminated at Wed Dec  6 15:47:44 2023
Results reported at Wed Dec  6 15:47:44 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J FFNN_baseline
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o ../Log_out_files/FFNN.out
#BSUB -e ../Log_out_files/FFNN.err

module load python3/3.11.4
module load h5py
python3 "CSL_FFNN_BS_1HL.py"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14708.16 sec.
    Max Memory :                                 692 MB
    Average Memory :                             581.85 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               32076.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   3951 sec.
    Turnaround time :                            3923 sec.

The output (if any) is above this job summary.



PS:

Read file <../Log_out_files/FFNN.err> for stderr output of this job.

