
------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19782955: <VAE_to_AE> in cluster <dcc> Exited

Job <VAE_to_AE> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Thu Dec 14 23:58:59 2023
Job was executed on host(s) <n-62-20-4>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Thu Dec 14 23:58:59 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/final_scripts> was used as the working directory.
Started at Thu Dec 14 23:58:59 2023
Terminated at Thu Dec 14 23:59:28 2023
Results reported at Thu Dec 14 23:59:28 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J VAE_to_AE 
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o FFNN.out
#BSUB -e FFNN.err

module load python3/3.11.4
module load h5py
python3 "FFNN_VAE_to_AE.py"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   4.52 sec.
    Max Memory :                                 158 MB
    Average Memory :                             158.00 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               32610.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   89 sec.
    Turnaround time :                            29 sec.

The output (if any) is above this job summary.



PS:

Read file <FFNN.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19783166: <VAE_to_AE> in cluster <dcc> Exited

Job <VAE_to_AE> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Fri Dec 15 00:18:16 2023
Job was executed on host(s) <n-62-20-4>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Fri Dec 15 00:18:16 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/final_scripts> was used as the working directory.
Started at Fri Dec 15 00:18:16 2023
Terminated at Fri Dec 15 00:18:22 2023
Results reported at Fri Dec 15 00:18:22 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J VAE_to_AE 
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o FFNN.out
#BSUB -e FFNN.err

module load python3/3.11.4
module load h5py
python3 "FFNN_VAE_to_AE.py"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   3.59 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   107 sec.
    Turnaround time :                            6 sec.

The output (if any) is above this job summary.



PS:

Read file <FFNN.err> for stderr output of this job.


Starting Fold 1:
Train samples: 13884, Test samples: 3472

Epoch 5: train loss (last batch), mean MSE:	0.4322,	0.42213
Epoch 5: valid loss (last batch), mean MSE:	0.3904,	0.42150
Epoch 5: valid baseline mean MSE:	0.46963

Epoch 10: train loss (last batch), mean MSE:	0.4108,	0.41663
Epoch 10: valid loss (last batch), mean MSE:	0.4774,	0.41742
Epoch 10: valid baseline mean MSE:	0.47048

Epoch 15: train loss (last batch), mean MSE:	0.4292,	0.40910
Epoch 15: valid loss (last batch), mean MSE:	0.3589,	0.40780
Epoch 15: valid baseline mean MSE:	0.46893

Epoch 20: train loss (last batch), mean MSE:	0.4231,	0.39489
Epoch 20: valid loss (last batch), mean MSE:	0.3664,	0.39507
Epoch 20: valid baseline mean MSE:	0.46839

Epoch 25: train loss (last batch), mean MSE:	0.3799,	0.38463
Epoch 25: valid loss (last batch), mean MSE:	0.4265,	0.38625
Epoch 25: valid baseline mean MSE:	0.46908

Epoch 30: train loss (last batch), mean MSE:	0.3529,	0.37908
Epoch 30: valid loss (last batch), mean MSE:	0.3659,	0.37781
Epoch 30: valid baseline mean MSE:	0.46884

Epoch 35: train loss (last batch), mean MSE:	0.3591,	0.37419
Epoch 35: valid loss (last batch), mean MSE:	0.4239,	0.37395
Epoch 35: valid baseline mean MSE:	0.46967

Epoch 40: train loss (last batch), mean MSE:	0.4130,	0.36868
Epoch 40: valid loss (last batch), mean MSE:	0.3375,	0.36705
Epoch 40: valid baseline mean MSE:	0.46885

Epoch 45: train loss (last batch), mean MSE:	0.4016,	0.36208
Epoch 45: valid loss (last batch), mean MSE:	0.4337,	0.36157
Epoch 45: valid baseline mean MSE:	0.47067

Epoch 50: train loss (last batch), mean MSE:	0.3712,	0.35597
Epoch 50: valid loss (last batch), mean MSE:	0.4307,	0.35675
Epoch 50: valid baseline mean MSE:	0.46990

Starting Fold 2:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4613,	0.42217
Epoch 5: valid loss (last batch), mean MSE:	0.4244,	0.42077
Epoch 5: valid baseline mean MSE:	0.46882

Epoch 10: train loss (last batch), mean MSE:	0.4091,	0.41628
Epoch 10: valid loss (last batch), mean MSE:	0.4842,	0.41593
Epoch 10: valid baseline mean MSE:	0.46947

Epoch 15: train loss (last batch), mean MSE:	0.3887,	0.40424
Epoch 15: valid loss (last batch), mean MSE:	0.3831,	0.40150
Epoch 15: valid baseline mean MSE:	0.46853

Epoch 20: train loss (last batch), mean MSE:	0.3641,	0.38783
Epoch 20: valid loss (last batch), mean MSE:	0.4084,	0.38648
Epoch 20: valid baseline mean MSE:	0.46842

Epoch 25: train loss (last batch), mean MSE:	0.3990,	0.37945
Epoch 25: valid loss (last batch), mean MSE:	0.3449,	0.37716
Epoch 25: valid baseline mean MSE:	0.46887

Epoch 30: train loss (last batch), mean MSE:	0.3695,	0.37289
Epoch 30: valid loss (last batch), mean MSE:	0.3691,	0.37133
Epoch 30: valid baseline mean MSE:	0.46919

Epoch 35: train loss (last batch), mean MSE:	0.3489,	0.36650
Epoch 35: valid loss (last batch), mean MSE:	0.4023,	0.36521
Epoch 35: valid baseline mean MSE:	0.47008

Epoch 40: train loss (last batch), mean MSE:	0.3399,	0.36011
Epoch 40: valid loss (last batch), mean MSE:	0.3477,	0.35763
Epoch 40: valid baseline mean MSE:	0.46870

Epoch 45: train loss (last batch), mean MSE:	0.3458,	0.35441
Epoch 45: valid loss (last batch), mean MSE:	0.4142,	0.35465
Epoch 45: valid baseline mean MSE:	0.47001

Epoch 50: train loss (last batch), mean MSE:	0.3322,	0.35029
Epoch 50: valid loss (last batch), mean MSE:	0.3283,	0.34849
Epoch 50: valid baseline mean MSE:	0.46869

Starting Fold 3:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4168,	0.42207
Epoch 5: valid loss (last batch), mean MSE:	0.3880,	0.42024
Epoch 5: valid baseline mean MSE:	0.47004

Epoch 10: train loss (last batch), mean MSE:	0.4031,	0.41591
Epoch 10: valid loss (last batch), mean MSE:	0.3878,	0.41389
Epoch 10: valid baseline mean MSE:	0.47034

Epoch 15: train loss (last batch), mean MSE:	0.4090,	0.40260
Epoch 15: valid loss (last batch), mean MSE:	0.3843,	0.39880
Epoch 15: valid baseline mean MSE:	0.46949

Epoch 20: train loss (last batch), mean MSE:	0.3538,	0.38648
Epoch 20: valid loss (last batch), mean MSE:	0.3786,	0.38499
Epoch 20: valid baseline mean MSE:	0.47027

Epoch 25: train loss (last batch), mean MSE:	0.3681,	0.37800
Epoch 25: valid loss (last batch), mean MSE:	0.3136,	0.37482
Epoch 25: valid baseline mean MSE:	0.46965

Epoch 30: train loss (last batch), mean MSE:	0.3674,	0.37147
Epoch 30: valid loss (last batch), mean MSE:	0.3798,	0.36963
Epoch 30: valid baseline mean MSE:	0.47103

Epoch 35: train loss (last batch), mean MSE:	0.3673,	0.36449
Epoch 35: valid loss (last batch), mean MSE:	0.3150,	0.36154
Epoch 35: valid baseline mean MSE:	0.46944

Epoch 40: train loss (last batch), mean MSE:	0.3626,	0.35841
Epoch 40: valid loss (last batch), mean MSE:	0.4176,	0.35695
Epoch 40: valid baseline mean MSE:	0.47158

Epoch 45: train loss (last batch), mean MSE:	0.3549,	0.35275
Epoch 45: valid loss (last batch), mean MSE:	0.3668,	0.35100
Epoch 45: valid baseline mean MSE:	0.46985

Epoch 50: train loss (last batch), mean MSE:	0.3689,	0.34860
Epoch 50: valid loss (last batch), mean MSE:	0.3665,	0.34691
Epoch 50: valid baseline mean MSE:	0.46981

Starting Fold 4:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4638,	0.42251
Epoch 5: valid loss (last batch), mean MSE:	0.4519,	0.42065
Epoch 5: valid baseline mean MSE:	0.46777

Epoch 10: train loss (last batch), mean MSE:	0.3914,	0.41690
Epoch 10: valid loss (last batch), mean MSE:	0.3505,	0.41410
Epoch 10: valid baseline mean MSE:	0.46558

Epoch 15: train loss (last batch), mean MSE:	0.3899,	0.40831
Epoch 15: valid loss (last batch), mean MSE:	0.4476,	0.40615
Epoch 15: valid baseline mean MSE:	0.46818

Epoch 20: train loss (last batch), mean MSE:	0.3753,	0.39337
Epoch 20: valid loss (last batch), mean MSE:	0.4051,	0.39089
Epoch 20: valid baseline mean MSE:	0.46803

Epoch 25: train loss (last batch), mean MSE:	0.3683,	0.38305
Epoch 25: valid loss (last batch), mean MSE:	0.4076,	0.38130
Epoch 25: valid baseline mean MSE:	0.46799

Epoch 30: train loss (last batch), mean MSE:	0.3870,	0.37595
Epoch 30: valid loss (last batch), mean MSE:	0.3469,	0.37373
Epoch 30: valid baseline mean MSE:	0.46695

Epoch 35: train loss (last batch), mean MSE:	0.3795,	0.36927
Epoch 35: valid loss (last batch), mean MSE:	0.3652,	0.36786
Epoch 35: valid baseline mean MSE:	0.46634

Epoch 40: train loss (last batch), mean MSE:	0.3868,	0.36290
Epoch 40: valid loss (last batch), mean MSE:	0.3644,	0.36273
Epoch 40: valid baseline mean MSE:	0.46606

Epoch 45: train loss (last batch), mean MSE:	0.3469,	0.35741
Epoch 45: valid loss (last batch), mean MSE:	0.3578,	0.35676
Epoch 45: valid baseline mean MSE:	0.46688

Epoch 50: train loss (last batch), mean MSE:	0.3406,	0.35327
Epoch 50: valid loss (last batch), mean MSE:	0.3934,	0.35261
Epoch 50: valid baseline mean MSE:	0.46719

Starting Fold 5:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	0.4153,	0.42170
Epoch 5: valid loss (last batch), mean MSE:	0.3762,	0.42287
Epoch 5: valid baseline mean MSE:	0.46721

Epoch 10: train loss (last batch), mean MSE:	0.4152,	0.41621
Epoch 10: valid loss (last batch), mean MSE:	0.3955,	0.41747
Epoch 10: valid baseline mean MSE:	0.46779

Epoch 15: train loss (last batch), mean MSE:	0.4029,	0.40771
Epoch 15: valid loss (last batch), mean MSE:	0.3974,	0.40934
Epoch 15: valid baseline mean MSE:	0.46663

Epoch 20: train loss (last batch), mean MSE:	0.4371,	0.39022
Epoch 20: valid loss (last batch), mean MSE:	0.4029,	0.39283
Epoch 20: valid baseline mean MSE:	0.46747

Epoch 25: train loss (last batch), mean MSE:	0.3807,	0.37944
Epoch 25: valid loss (last batch), mean MSE:	0.3766,	0.38075
Epoch 25: valid baseline mean MSE:	0.46700

Epoch 30: train loss (last batch), mean MSE:	0.3823,	0.37154
Epoch 30: valid loss (last batch), mean MSE:	0.4132,	0.37351
Epoch 30: valid baseline mean MSE:	0.46844

Epoch 35: train loss (last batch), mean MSE:	0.3657,	0.36404
Epoch 35: valid loss (last batch), mean MSE:	0.3343,	0.36540
Epoch 35: valid baseline mean MSE:	0.46666

Epoch 40: train loss (last batch), mean MSE:	0.3351,	0.35707
Epoch 40: valid loss (last batch), mean MSE:	0.3823,	0.35948
Epoch 40: valid baseline mean MSE:	0.46766

Epoch 45: train loss (last batch), mean MSE:	0.3480,	0.35276
Epoch 45: valid loss (last batch), mean MSE:	0.3825,	0.35505
Epoch 45: valid baseline mean MSE:	0.46753

Epoch 50: train loss (last batch), mean MSE:	0.3298,	0.34940
Epoch 50: valid loss (last batch), mean MSE:	0.4187,	0.35320
Epoch 50: valid baseline mean MSE:	0.46793

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19783187: <VAE_to_AE> in cluster <dcc> Done

Job <VAE_to_AE> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Fri Dec 15 00:21:16 2023
Job was executed on host(s) <n-62-20-4>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Fri Dec 15 00:21:16 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/final_scripts> was used as the working directory.
Started at Fri Dec 15 00:21:16 2023
Terminated at Fri Dec 15 01:32:08 2023
Results reported at Fri Dec 15 01:32:08 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J VAE_to_AE 
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o FFNN.out
#BSUB -e FFNN.err

module load python3/3.11.4
module load h5py
python3 "FFNN_VAE_to_AE.py"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   16082.54 sec.
    Max Memory :                                 781 MB
    Average Memory :                             637.10 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               31987.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   4330 sec.
    Turnaround time :                            4252 sec.

The output (if any) is above this job summary.



PS:

Read file <FFNN.err> for stderr output of this job.


Starting Fold 1:
Train samples: 13884, Test samples: 3472

Epoch 5: train loss (last batch), mean MSE:	1.3948,	1.44752
Epoch 5: valid loss (last batch), mean MSE:	1.2982,	1.37869
Epoch 5: valid baseline mean MSE:	0.46909

Epoch 10: train loss (last batch), mean MSE:	0.8781,	0.95832
Epoch 10: valid loss (last batch), mean MSE:	0.9983,	0.92731
Epoch 10: valid baseline mean MSE:	0.46960

Epoch 15: train loss (last batch), mean MSE:	0.7001,	0.71869
Epoch 15: valid loss (last batch), mean MSE:	0.7042,	0.70287
Epoch 15: valid baseline mean MSE:	0.46895

Epoch 20: train loss (last batch), mean MSE:	0.5940,	0.59815
Epoch 20: valid loss (last batch), mean MSE:	0.5314,	0.58966
Epoch 20: valid baseline mean MSE:	0.46860

Epoch 25: train loss (last batch), mean MSE:	0.5258,	0.53774
Epoch 25: valid loss (last batch), mean MSE:	0.5138,	0.53392
Epoch 25: valid baseline mean MSE:	0.46899

Epoch 30: train loss (last batch), mean MSE:	0.5288,	0.50761
Epoch 30: valid loss (last batch), mean MSE:	0.5048,	0.50610
Epoch 30: valid baseline mean MSE:	0.46874

Epoch 35: train loss (last batch), mean MSE:	0.4858,	0.49241
Epoch 35: valid loss (last batch), mean MSE:	0.4780,	0.49174
Epoch 35: valid baseline mean MSE:	0.46921

Epoch 40: train loss (last batch), mean MSE:	0.4806,	0.48459
Epoch 40: valid loss (last batch), mean MSE:	0.4585,	0.48426
Epoch 40: valid baseline mean MSE:	0.46854

Epoch 45: train loss (last batch), mean MSE:	0.4279,	0.48051
Epoch 45: valid loss (last batch), mean MSE:	0.5055,	0.48113
Epoch 45: valid baseline mean MSE:	0.46929

Epoch 50: train loss (last batch), mean MSE:	0.5325,	0.47842
Epoch 50: valid loss (last batch), mean MSE:	0.5530,	0.47983
Epoch 50: valid baseline mean MSE:	0.47022

Starting Fold 2:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.2490,	1.44800
Epoch 5: valid loss (last batch), mean MSE:	1.4847,	1.37600
Epoch 5: valid baseline mean MSE:	0.46961

Epoch 10: train loss (last batch), mean MSE:	0.9536,	0.95847
Epoch 10: valid loss (last batch), mean MSE:	0.9647,	0.92373
Epoch 10: valid baseline mean MSE:	0.46886

Epoch 15: train loss (last batch), mean MSE:	0.6813,	0.71883
Epoch 15: valid loss (last batch), mean MSE:	0.6529,	0.70045
Epoch 15: valid baseline mean MSE:	0.46881

Epoch 20: train loss (last batch), mean MSE:	0.6175,	0.59831
Epoch 20: valid loss (last batch), mean MSE:	0.5594,	0.58909
Epoch 20: valid baseline mean MSE:	0.46856

Epoch 25: train loss (last batch), mean MSE:	0.5183,	0.53789
Epoch 25: valid loss (last batch), mean MSE:	0.5361,	0.53371
Epoch 25: valid baseline mean MSE:	0.46969

Epoch 30: train loss (last batch), mean MSE:	0.5192,	0.50773
Epoch 30: valid loss (last batch), mean MSE:	0.5066,	0.50582
Epoch 30: valid baseline mean MSE:	0.46893

Epoch 35: train loss (last batch), mean MSE:	0.4546,	0.49250
Epoch 35: valid loss (last batch), mean MSE:	0.4044,	0.49049
Epoch 35: valid baseline mean MSE:	0.46781

Epoch 40: train loss (last batch), mean MSE:	0.4790,	0.48468
Epoch 40: valid loss (last batch), mean MSE:	0.4752,	0.48430
Epoch 40: valid baseline mean MSE:	0.46913

Epoch 45: train loss (last batch), mean MSE:	0.4706,	0.48059
Epoch 45: valid loss (last batch), mean MSE:	0.4988,	0.48089
Epoch 45: valid baseline mean MSE:	0.46916

Epoch 50: train loss (last batch), mean MSE:	0.4868,	0.47846
Epoch 50: valid loss (last batch), mean MSE:	0.5192,	0.47924
Epoch 50: valid baseline mean MSE:	0.46945

Starting Fold 3:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.3969,	1.44708
Epoch 5: valid loss (last batch), mean MSE:	1.6658,	1.38048
Epoch 5: valid baseline mean MSE:	0.47051

Epoch 10: train loss (last batch), mean MSE:	0.9489,	0.95847
Epoch 10: valid loss (last batch), mean MSE:	0.9537,	0.92550
Epoch 10: valid baseline mean MSE:	0.47135

Epoch 15: train loss (last batch), mean MSE:	0.7222,	0.71884
Epoch 15: valid loss (last batch), mean MSE:	0.6680,	0.70234
Epoch 15: valid baseline mean MSE:	0.47049

Epoch 20: train loss (last batch), mean MSE:	0.6098,	0.59820
Epoch 20: valid loss (last batch), mean MSE:	0.5851,	0.59100
Epoch 20: valid baseline mean MSE:	0.46985

Epoch 25: train loss (last batch), mean MSE:	0.4920,	0.53768
Epoch 25: valid loss (last batch), mean MSE:	0.5068,	0.53479
Epoch 25: valid baseline mean MSE:	0.47020

Epoch 30: train loss (last batch), mean MSE:	0.4965,	0.50746
Epoch 30: valid loss (last batch), mean MSE:	0.5706,	0.50819
Epoch 30: valid baseline mean MSE:	0.47098

Epoch 35: train loss (last batch), mean MSE:	0.5055,	0.49221
Epoch 35: valid loss (last batch), mean MSE:	0.4612,	0.49276
Epoch 35: valid baseline mean MSE:	0.47002

Epoch 40: train loss (last batch), mean MSE:	0.4580,	0.48434
Epoch 40: valid loss (last batch), mean MSE:	0.4480,	0.48541
Epoch 40: valid baseline mean MSE:	0.46997

Epoch 45: train loss (last batch), mean MSE:	0.4781,	0.48025
Epoch 45: valid loss (last batch), mean MSE:	0.4884,	0.48221
Epoch 45: valid baseline mean MSE:	0.47071

Epoch 50: train loss (last batch), mean MSE:	0.4455,	0.47812
Epoch 50: valid loss (last batch), mean MSE:	0.4380,	0.47957
Epoch 50: valid baseline mean MSE:	0.46961

Starting Fold 4:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.3766,	1.44599
Epoch 5: valid loss (last batch), mean MSE:	1.5589,	1.37880
Epoch 5: valid baseline mean MSE:	0.46757

Epoch 10: train loss (last batch), mean MSE:	0.9730,	0.95767
Epoch 10: valid loss (last batch), mean MSE:	1.0309,	0.92515
Epoch 10: valid baseline mean MSE:	0.46718

Epoch 15: train loss (last batch), mean MSE:	0.6973,	0.71860
Epoch 15: valid loss (last batch), mean MSE:	0.7954,	0.70201
Epoch 15: valid baseline mean MSE:	0.46750

Epoch 20: train loss (last batch), mean MSE:	0.5790,	0.59833
Epoch 20: valid loss (last batch), mean MSE:	0.6392,	0.58917
Epoch 20: valid baseline mean MSE:	0.46747

Epoch 25: train loss (last batch), mean MSE:	0.5130,	0.53808
Epoch 25: valid loss (last batch), mean MSE:	0.6244,	0.53351
Epoch 25: valid baseline mean MSE:	0.46896

Epoch 30: train loss (last batch), mean MSE:	0.5189,	0.50805
Epoch 30: valid loss (last batch), mean MSE:	0.4813,	0.50387
Epoch 30: valid baseline mean MSE:	0.46700

Epoch 35: train loss (last batch), mean MSE:	0.5199,	0.49291
Epoch 35: valid loss (last batch), mean MSE:	0.5060,	0.49024
Epoch 35: valid baseline mean MSE:	0.46752

Epoch 40: train loss (last batch), mean MSE:	0.5215,	0.48511
Epoch 40: valid loss (last batch), mean MSE:	0.4547,	0.48232
Epoch 40: valid baseline mean MSE:	0.46658

Epoch 45: train loss (last batch), mean MSE:	0.4797,	0.48103
Epoch 45: valid loss (last batch), mean MSE:	0.4762,	0.47885
Epoch 45: valid baseline mean MSE:	0.46744

Epoch 50: train loss (last batch), mean MSE:	0.5029,	0.47892
Epoch 50: valid loss (last batch), mean MSE:	0.4134,	0.47602
Epoch 50: valid baseline mean MSE:	0.46633

Starting Fold 5:
Train samples: 13885, Test samples: 3471

Epoch 5: train loss (last batch), mean MSE:	1.3701,	1.44466
Epoch 5: valid loss (last batch), mean MSE:	1.4364,	1.38531
Epoch 5: valid baseline mean MSE:	0.46868

Epoch 10: train loss (last batch), mean MSE:	0.9610,	0.95729
Epoch 10: valid loss (last batch), mean MSE:	0.8912,	0.92871
Epoch 10: valid baseline mean MSE:	0.46692

Epoch 15: train loss (last batch), mean MSE:	0.6804,	0.71852
Epoch 15: valid loss (last batch), mean MSE:	0.7257,	0.70457
Epoch 15: valid baseline mean MSE:	0.46754

Epoch 20: train loss (last batch), mean MSE:	0.5947,	0.59836
Epoch 20: valid loss (last batch), mean MSE:	0.5708,	0.59045
Epoch 20: valid baseline mean MSE:	0.46739

Epoch 25: train loss (last batch), mean MSE:	0.5198,	0.53810
Epoch 25: valid loss (last batch), mean MSE:	0.5804,	0.53435
Epoch 25: valid baseline mean MSE:	0.46864

Epoch 30: train loss (last batch), mean MSE:	0.5286,	0.50805
Epoch 30: valid loss (last batch), mean MSE:	0.5322,	0.50556
Epoch 30: valid baseline mean MSE:	0.46812

Epoch 35: train loss (last batch), mean MSE:	0.4900,	0.49287
Epoch 35: valid loss (last batch), mean MSE:	0.5178,	0.49113
Epoch 35: valid baseline mean MSE:	0.46752

Epoch 40: train loss (last batch), mean MSE:	0.4705,	0.48504
Epoch 40: valid loss (last batch), mean MSE:	0.3952,	0.48206
Epoch 40: valid baseline mean MSE:	0.46653

Epoch 45: train loss (last batch), mean MSE:	0.4395,	0.48094
Epoch 45: valid loss (last batch), mean MSE:	0.4955,	0.47962
Epoch 45: valid baseline mean MSE:	0.46832

Epoch 50: train loss (last batch), mean MSE:	0.4581,	0.47882
Epoch 50: valid loss (last batch), mean MSE:	0.4571,	0.47709
Epoch 50: valid baseline mean MSE:	0.46657

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 19793530: <VAE_to_AE> in cluster <dcc> Done

Job <VAE_to_AE> was submitted from host <hpclogin1> by user <s222797> in cluster <dcc> at Sat Dec 16 00:46:16 2023
Job was executed on host(s) <n-62-11-13>, in queue <gpuv100>, as user <s222797> in cluster <dcc> at Sat Dec 16 00:46:17 2023
</zhome/e7/a/181331> was used as the home directory.
</zhome/e7/a/181331/deep_learning_project/final_scripts> was used as the working directory.
Started at Sat Dec 16 00:46:17 2023
Terminated at Sat Dec 16 02:05:41 2023
Results reported at Sat Dec 16 02:05:41 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -q gpuv100
#BSUB -gpu "num=1"
#BSUB -J VAE_to_AE 
#BSUB -n 1
#BSUB -W 10:00
#BSUB -R "rusage[mem=32GB]"
#BSUB -o FFNN.out
#BSUB -e FFNN.err

module load python3/3.11.4
module load h5py
python3 "FFNN_VAE_to_AE.py"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   16260.60 sec.
    Max Memory :                                 789 MB
    Average Memory :                             620.85 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               31979.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   4801 sec.
    Turnaround time :                            4765 sec.

The output (if any) is above this job summary.



PS:

Read file <FFNN.err> for stderr output of this job.

